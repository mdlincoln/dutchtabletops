---
title: "Form and Content in 17th-Century Haarlem Still Lifes"
author: "Matthew Lincoln"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Form and Content in 17th-Century Haarlem Still Lifes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r libraries, include = FALSE}
library(dutchtabletopsdata)
library(purrr)
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(ggrepel)
library(randomForest)
```

## What are random forests? Why use them?

Random forests are an example of an _ensemble machine learning method_ that works by running many iterations of the same type of model and combining their results in an effort to avoid overfitting.
The core model of random forests are, appropriately, decision trees.
A single decision tree works by finding a split in one of the _predictor variables_ (say, all horizontal paintings over here, all horiztonal ones over here) that does the best job at sorting out paintings based on the given _repsonse variable_ (e.g., which artist painted it.), techinically known as asessing its _node purity_.
One variable split likely does a mediocre job of predicting the response variable, thus the process is repeated, finding the best variable splits for each one of the branches produced by the first split, then the best splits for all the brances produced by that step, and so forth.
Single decision trees can keep producing branches until they have perfectly memorized the data.
Much like an undergraduate who aces the slide ID portion of the art history exam, but bombs the unknown image section, a single tree will perfectly reproduce the data you give it, but does poorly when given new cases that might have slightly different attributes than those it learned from.

Random forests work by creating hundreds or thousands of decision trees, giving each a slightly different subset of the original data to learn. Then, having built each of these trees, the model polls this "forest" in order to classify new data into different categories.
In the case of a model learning to recognize artists based on the motifs used in paintings, some paintings may elicit an overwhelmingly lopsided decision by the forest when it is particularly sure of which attribution to give.
Other paintings, however, may provoke more confusion - their attribution, in other words, may not be supportable based on motif information _alone_.

Random forests are particularly appropriate for our use for several reasons:

1. They work relatively well on small samples of data that have a large number of descriptive variables
1. Random forests performs well on systems where there are complex interactions between variables (e.g., lemon plus wine but _without_ oysters tends to be a painitng by...)

## Building the random forests

There are three main parameters to any of these models:

1. Predictor variables: what variables will the model use to make that prediction?
  1. Motif variables only
  1. Composition variables only
    - With dimension informaion? Without?
  1. All available variables
1. Response variable: what categorical or scalar value are we attempting to predict?
  1. An unqualified artist attribution (e.g "Pieter Claesz")
  1. A qualified attribution ("after Pieter Claesz", "possibly Pieter Claesz")
  1. Some other variable, e.g. general compositional scheme
1. Data subset: What subset of the available observations (in this case, paintings) will the model use to make predictions? Just those paintings with definitve attributions? All available paintings? Paintings from a given period of time?

Because we may be interested in any number of combinations of these three parameters, we'll produce random forests for every combination up front, allowing easier exploration on the back end.

```{r model_specs, message=FALSE}
# Model artists with only n or more paintings
n_ptg <- 10
cleared_artists <- unique(filter(group_by(dt_compiled, artist), n() >= n_ptg)$artist)
cleared_artist_union <- unique(filter(group_by(dt_compiled, artist_union), n() >= n_ptg)$artist_union)

dt_valid <- dt_compiled %>% 
  filter(artist %in% cleared_artists, artist_union %in% cleared_artist_union)

# Model specifications
model_predictors <- list(
  motif_only = motif_vars(),
  composition_only = comp_vars(),
  comp_nodim = setdiff(comp_vars(), c("height", "width")),
  combined = c(motif_vars(), comp_vars())
)

model_response <- list(
  simple = "artist",
  # qualified = "artist_union",
  comp_disp = "compositional_disposition",
  year = "year_early"
)

model_subsets <- list(
  definite_only = which(dt_valid$artist_relationship == "Definite" & dt_valid$artist_union %in% cleared_artist_union & dt_valid$artist %in% cleared_artists)
)

# Produce all combinations (the dot product) of these three model parameters
model_params <- list(predictors = model_predictors, response = model_response, subsets = model_subsets) %>% 
  cross_named_lists()

# Actually run the models
rfl <- map(model_params, function(x) run_rf(dt_valid, response = x$response, predictors = x$predictors, ntree = 5000, portion = x$subsets))

# Add the resulting random forest type ("classification" or "regression") to the
# model name
names(rfl) <- map2_chr(rfl, names(rfl), function(x, y) {
  type <- x$type
  paste(y, type, sep = ".")
})
```

## Which artists were predictable by their chosen motifs? Their chosen compositions?

One core question: are certain artists more or less predictable by the motifs they choose, and/or by the compositions they use to arrange those motifs?

```{r class_errors}
# Produce a long table of model error: how often was a given model "wrong" about
# the category in which it placed an artwork?
rfl_df <- rfl %>% 
  keep(function(x) x$type == "classification") %>% 
  map_df(error_only, .id = "model_type") %>% 
  separate(model_type, into = c("predictors", "response", "subset", "forest_type"), sep = "\\.")
```

```{r plot_errors}
per_artist_error <- rfl_df %>% 
  filter(response == "simple" & subset == "definite_only" & predictors %in% c("motif_only", "composition_only")) %>% 
  select(-response, -forest_type) %>% 
  ungroup() %>% 
  spread(predictors, class.error) %>% 
  rename(artist = actual)

high <- 0.9
low <- 0.1

pae <- ggplot(per_artist_error, aes(x = composition_only, y = motif_only, color = artist)) +
  annotate("rect", xmin = 0.5, xmax = Inf, ymin = 0.5, ymax = Inf, alpha = 0.2) +
  annotate("rect", xmin = -Inf, xmax = 0.5, ymin = -Inf, ymax = 0.5, alpha = 0.2) +
  annotate("text", label = "Unpredictable Composition\nUnpredictable Motifs", x = high, y = high) +
  annotate("text", label = "Predictable Composition\nUnpredictable Motifs", x = low, y = high) +
  annotate("text", label = "Predictable Composition\nPredictable Motifs", x = low, y = low) +
  annotate("text", label = "Unpredictable Composition\nPredictable Motifs", x = high, y = low) +
  geom_point(size = 5) +
  geom_smooth() +
  theme_bw() +
  geom_label_repel(aes(label = artist)) +
  scale_colour_brewer(palette = "Dark2", guide = FALSE) +
  theme(legend.position = "top") +
  xlim(0, 1) + ylim(0, 1) +
  labs(x = "Composition predictors", y = "Motif predictors", shape = "Attribution qualification")

multiplot(pae, height = 7, width = 10)
```

Several points of note:

1. Those paintings whose attribution is qualified as "Possible" are the most confusing to the model; both Willem Claesz Heda and Pieter Claes "possible" paintings have at or near 100% error based on either motif or compositional predictors.
1. Gerrit Heda is a mercurial painter: of the "Definite" attributions, the model is mistaken more that 75% of the time when trying to attribute them on either motif or compositional predictors. Cornelis Mahu is also unpredictable by both counts.
1. Floris van Dijck and Floris van Schooten, on the other hand, are relatively more precitable, with Van Dijck's compositions distinct enough that the model can predict his paintings with 90% accuracy when looking only at compositions, and Van Schooten's works with about 80% accuracy when just looking at motifs.

Things get complex when looking at the difference between unqualified Willem Claesz Heda and Pieter Claesz attributions, and those attributions that are "after" the style of WCH or PC.
Known paintings by Pieter Claesz are predictable by their motifs - the most predictable of any class of paintings in the database, and relatively predictable by their composiitons.
On the other hand, those paintings done in the style of Pieter Claesz by a student or imitator have a much wider range of composiitonal arrangmenets that render them unpredictable by composition alone.
"After Pieter Claesz" paintings are somewhat predictable by motif, but much less so than their definitively-attributed counterparts.
What is more, their compositions are far more heterodox, suggesting that imitators of Claesz paid little heed to his particular modes of arranging objects, and only some attention to the types of objects he commonly included in his scenes.
The market for Pieter Claesz-like paintings apparently had little interest in the unique objects or compositions that Claesz himself 

Not so for Willem Claesz Heda derivatives.
Paintings after Heda are actally slightly _more_ predictable than the definitely-identified ones, at least when judging by their motifs.
Whereas derivatives of Claesz had a very wide visual range, those of Heda were much more specific.
The idea of a "Heda" painting was apparently more constricted than was the conception of a "Claesz".

This difference points to the complexity of this seemingly-bounded genre of painting production.
Claesz and Heda were contemporaries, thus we are not 

## Comparing known to unknown quantities

Which artists does this model understand when only learning those definite paintings?

```{r test_known_only, eval = FALSE}
known_only_motif <- rfl$motif_only.simple.definite_only.classification
known_only_comp <- rfl$composition_only.simple.definite_only.classification

run_prediction_error <- function(model) {
  pv <- dt_valid %>% 
    filter(artist %in% cleared_artists, artist_relationship != "Definite") %>%
    select(one_of(c("artist", "painting_code", motif_vars())))
    prep_vars() %>% 
    na.roughfix() %>% 
    predict(model, newdata = .)
  
  pvd <- data_frame(predicted = as.character(pv)) %>% 
    mutate(painting_code = names(pv))
  
  dt_valid %>% 
    select(painting_code, artist, artist_relationship) %>% 
    inner_join(pvd, by = "painting_code") %>% 
    mutate(incorrect = artist != predicted) %>% 
    group_by(artist, artist_relationship) %>% 
    summarize(error = mean(incorrect)) %>% 
    ungroup() %>% 
    complete(artist, artist_relationship)
}

# known_only_motif %>% 
#   run_prediction_error() %>% 
#   ggplot(aes(x = artist, y = error, fill = artist_relationship)) +
#   geom_bar(stat = "identity", position = "dodge")
# 
# known_only_comp %>% 
#   run_prediction_error() %>% 
#   ggplot(aes(x = artist, y = error, fill = artist_relationship)) +
#   geom_bar(stat = "identity", position = "dodge")

known_error <- bind_rows(
  "motif" = run_prediction_error(known_only_motif),
  "composition" = run_prediction_error(known_only_comp),
  .id = "type") %>% 
  spread(type, error) %>% 
  filter(artist_relationship %in% c("After", "Definite", "Possible"))

ggplot(known_error, aes(x = composition, y = motif, color = artist, shape = artist_relationship)) +
  geom_point(size = 5) +
  geom_label_repel(aes(label = artist)) +
  xlim(0, 1) + ylim(0, 1)
```

```{r known_only_test}
fctr_dt_compiled <- prep_vars(dt_valid)

known_artist_subset <- fctr_dt_compiled %>% 
  filter(artist_relationship == "Definite" & artist %in% cleared_artists)

test_predictors <- fctr_dt_compiled %>% 
  filter(artist_relationship != "Definite" & artist %in% known_artist_subset$artist)

rf_rn <- function(df, rownames, predictors) {
  df %>% 
    na.roughfix() %>% 
    select(one_of(c("artist", rownames, predictors))) %>% 
    column_to_rownames(var = rownames)
}

kunk_rf <- function(train, test, predictors) {
  train_df <- rf_rn(train, "painting_code", predictors)
  test_df <- rf_rn(test, "painting_code", predictors)
  train_response <- as.factor(as.character(train_df$artist))
  test_response <- factor(as.character(test_df$artist), levels = levels(train_response))
  train_df$artist <- NULL
  test_df$artist <- NULL
  
  randomForest(x = train_df, y = train_response, xtest = test_df, ytest = test_response, ntree = 5000, importance = TRUE, localImp = TRUE, proximity = TRUE)
}

motif_trf <- kunk_rf(train = known_artist_subset, test = test_predictors, predictors = motif_vars())
comp_trf <- kunk_rf(train = known_artist_subset, test = test_predictors, predictors = comp_vars())

trf_votes <- bind_rows(
  motif = test_confusion(motif_trf, "painting_code", ytest = test_predictors$artist),
  comp = test_confusion(comp_trf, "painting_code", ytest = test_predictors$artist),
  .id = "predictor_type"
) %>% 
  inner_join(select(dt_compiled, painting_code, artist_relationship), by = "painting_code") %>% 
  group_by(predictor_type, actual, artist_relationship) %>% 
  summarize(error = 1 - mean(correct)) %>% 
  spread(predictor_type, error)

ggplot(trf_votes, aes(x = comp, y = motif, color = actual, shape = artist_relationship)) +
  geom_point(size = 5) +
  geom_label_repel(aes(label = actual)) +
  annotate("segment", x = 0.5, xend = 0.5, y = -Inf, yend = Inf) +
  annotate("segment", x = -Inf, xend = Inf, y = 0.5, yend = 0.5) +
  annotate("text", label = "Unpredictable Composition\nUnpredictable Motifs", x = 0.75, y = 0.75) +
  annotate("text", label = "Predictable Composition\nUnpredictable Motifs", x = 0.25, y = 0.75) +
  annotate("text", label = "Predictable Composition\nPredictable Motifs", x = 0.25, y = 0.25) +
  annotate("text", label = "Unpredictable Composition\nPredictable Motifs", x = 0.75, y = 0.25) +
  theme_bw() +
  xlim(0, 1) + ylim(0, 1)

```


## Local variable importance

```{r pca}
single_rf <- rfl$motif_only.simple.definite_only.classification

rfl_pca <- map(set_names(levels(single_rf$predicted)), function(x) {
  rf_local_importance(single_rf) %>% 
    semi_join(dt_compiled %>% filter(artist == x[1]), by = c("rowname" = "painting_code")) %>% 
    column_to_rownames("rowname") %>% 
    prcomp()
})

join_loadings <- function(pca) {
  pca_loadings_df(pca) %>% 
    left_join(dt_motif_labels, by = c("rowname" = "motif_code")) %>% 
    mutate(varlab = ifelse(is.na(wrapped_motif_label), rowname, motif_label))
}

rfl_loadings <- map(rfl_pca, join_loadings)

rfl_observations <- map(rfl_pca, pca_obs_df)


artist_motif_plot <- function(l, o, m = 12, hue = "red") {
  l %>% 
    filter(min_rank(desc(power)) <= m) %>% 
    ggplot(aes(x = PC1, y = PC2)) +
    geom_text(data = o, aes(label = rownames), alpha = 0.5) +
    geom_segment(aes(xend = 0, yend = 0), color = hue) +
    geom_label_repel(aes(label = varlab))
}

rfl_plots <- map2(rfl_loadings, rfl_observations, artist_motif_plot) %>% 
  map2(., names(rfl_observations), function(x, y) x + ggtitle(y))

```

```{r}
full_pca <- rfl$motif_only.simple.definite_only.classification %>% 
  rf_local_importance() %>% 
  column_to_rownames() %>% 
  prcomp()

full_plot <- artist_motif_plot(join_loadings(full_pca), pca_obs_df(full_pca))
plot(full_plot)
```

